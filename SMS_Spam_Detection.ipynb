{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataframe\n",
    "df=pd.read_csv(\"SMS Spam Filter.csv\")\n",
    "#renaming the column names\n",
    "df.columns=[\"Label\",\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the 1st 5 rows of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5568, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying dimensions of dataset in tuple format\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'Text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying all the column names of the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying all unqiue values of target column\n",
    "df[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ham Cases are:\n",
      "4822\n",
      "Number of Spam Cases are:\n",
      "746\n"
     ]
    }
   ],
   "source": [
    "#displaying count of each value of label\n",
    "print(\"Number of Ham Cases are:\")\n",
    "print(len(df[df[\"Label\"]==\"ham\"]))\n",
    "print(\"Number of Spam Cases are:\")\n",
    "print(len(df[df[\"Label\"]==\"spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#representing the counts in the count plot\n",
    "sns.countplot(x=df[\"Label\"],color=\"crimson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label    0\n",
       "Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if there is any missing value in the dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning/Preprocessing our dataset includes the follwoing\n",
    "#### 1) Remove Punctuation\n",
    "#### 2) Tokenization\n",
    "#### 3) Remove Stopwords\n",
    "#### 4) Stemming / Lemmatization\n",
    "#### 5) Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying all types of punctuation stored\n",
    "#since punctuations do not convey any meaning or sentiments so we can remove them\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Without Punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                 Without Punctuation  \n",
       "0  Ive been searching for the right words to than...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "2  Nah I dont think he goes to usf he lives aroun...  \n",
       "3  Even my brother is not like to speak with me T...  \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function created to remove punctuation\n",
    "def remove_punc(txt):\n",
    "    lst1=\"\".join([i for i in txt if i not in string.punctuation])\n",
    "    return lst1\n",
    "\n",
    "df[\"Without Punctuation\"]=df[\"Text\"].apply(lambda x:remove_punc(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Without Punctuation</th>\n",
       "      <th>After Tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                 Without Punctuation  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                  After Tokenization  \n",
       "0  [ive, been, searching, for, the, right, words,...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...  \n",
       "3  [even, my, brother, is, not, like, to, speak, ...  \n",
       "4         [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to do tokenization\n",
    "def tokenize(txt):\n",
    "    lst2=re.split('\\W+',txt)\n",
    "    return lst2\n",
    "\n",
    "df[\"After Tokenization\"]=df[\"Without Punctuation\"].apply(lambda x:tokenize(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing all the stopwords in english language\n",
    "sw=nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#since we took x.lower() in last case so we need not to consider any upper case alphabets\n",
    "#slicing 1st 10 contents of the list\n",
    "sw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Without Punctuation</th>\n",
       "      <th>After Tokenization</th>\n",
       "      <th>After Removing Stop Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                 Without Punctuation  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                  After Tokenization  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                           After Removing Stop Words  \n",
       "0  [ive, searching, right, words, thank, breather...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...  \n",
       "3  [even, brother, like, speak, treat, like, aids...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to remove stop words\n",
    "def remove_sw(lst2):\n",
    "    lst3=[i for i in lst2 if i not in sw]\n",
    "    return lst3\n",
    "\n",
    "df[\"After Removing Stop Words\"]=df[\"After Tokenization\"].apply(lambda x:remove_sw(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming removes the prefix or suffix part of any word to find the root word from the word\n",
    "ps=nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "grow\n",
      "grow\n"
     ]
    }
   ],
   "source": [
    "#all the following have same root word which is \"grow\"\n",
    "print(ps.stem(\"grows\"))\n",
    "print(ps.stem(\"growing\"))\n",
    "print(ps.stem(\"grow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#all the following have same root word which is \"run\"\n",
    "print(ps.stem(\"runs\"))\n",
    "print(ps.stem(\"running\"))\n",
    "print(ps.stem(\"run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Without Punctuation</th>\n",
       "      <th>After Tokenization</th>\n",
       "      <th>After Removing Stop Words</th>\n",
       "      <th>After Stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                 Without Punctuation  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                  After Tokenization  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                           After Removing Stop Words  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                      After Stemming  \n",
       "0  [ive, search, right, word, thank, breather, pr...  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goe, usf, live, around, tho...  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to do stemming\n",
    "def stemming(lst3):\n",
    "    lst4=[ps.stem(i) for i in lst3]\n",
    "    return lst4\n",
    "\n",
    "df[\"After Stemming\"]=df[\"After Removing Stop Words\"].apply(lambda x:stemming(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under stemming we get some errorful words and to fix them we use lemmatization\n",
    "wn=nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "mean\n"
     ]
    }
   ],
   "source": [
    "#though \"meanness\" and \"meaning\" have different definition still stem gives same root word as result\n",
    "print(ps.stem(\"meanness\"))\n",
    "print(ps.stem(\"meaning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanness\n",
      "meaning\n"
     ]
    }
   ],
   "source": [
    "#as \"meanness\" and \"meaning\" have different definition so lemmatize gives different root words as results\n",
    "print(wn.lemmatize(\"meanness\"))\n",
    "print(wn.lemmatize(\"meaning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gees\n",
      "goos\n"
     ]
    }
   ],
   "source": [
    "#though \"geese\" and \"goose\" have similar definition still stem gives different root words as result\n",
    "print(ps.stem(\"geese\"))\n",
    "print(ps.stem(\"goose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goose\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "#as \"geese\" and \"goose\" have similar definition so lemmatize gives same root word as result\n",
    "print(wn.lemmatize(\"geese\"))\n",
    "print(wn.lemmatize(\"goose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Without Punctuation</th>\n",
       "      <th>After Tokenization</th>\n",
       "      <th>After Removing Stop Words</th>\n",
       "      <th>After Stemming</th>\n",
       "      <th>After Lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, pr...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                 Without Punctuation  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                  After Tokenization  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                           After Removing Stop Words  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                      After Stemming  \\\n",
       "0  [ive, search, right, word, thank, breather, pr...   \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goe, usf, live, around, tho...   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     After Lemmatize  \n",
       "0  [ive, searching, right, word, thank, breather,...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, go, usf, life, around, though]  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to do lemmatization\n",
    "def lemmatizing(lst3):\n",
    "    lst4=[wn.lemmatize(i) for i in lst3]\n",
    "    return lst4\n",
    "\n",
    "df[\"After Lemmatize\"]=df[\"After Removing Stop Words\"].apply(lambda x:lemmatizing(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation, Tokenization, Removing Stopwords, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>After Cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>[I, search, right, word, thank, breather, I, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, I, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, they, treat, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I, have, A, date, ON, sunday, with, will, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                      After Cleaning  \n",
       "0  [I, search, right, word, thank, breather, I, p...  \n",
       "1  [free, entri, 2, wkli, comp, win, FA, cup, fin...  \n",
       "2    [nah, I, think, goe, usf, live, around, though]  \n",
       "3  [even, brother, like, speak, they, treat, like...  \n",
       "4       [I, have, A, date, ON, sunday, with, will, ]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    lst1=\"\".join([i for i in txt if i not in string.punctuation])\n",
    "    lst2=re.split('\\W+',txt)\n",
    "    lst3=[i for i in lst2 if i not in sw]\n",
    "    lst4=[ps.stem(i) for i in lst3]\n",
    "    return lst4\n",
    "\n",
    "df_new=pd.read_csv(\"SMS Spam Filter.tsv\",sep=\"\\t\",header=None)\n",
    "df_new.columns=[\"Label\",\"Text\"]\n",
    "df_new[\"After Cleaning\"]=df[\"Text\"].apply(lambda x:clean_text(x))\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5568, 7531)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#there are many parameters but analyzer is used as we need each row to be passed to the cv object\n",
    "#analyzer takes a function value and passes each row to that function\n",
    "cv=CountVectorizer(analyzer=clean_text)\n",
    "x=cv.fit_transform(df[\"Text\"])\n",
    "\n",
    "#number of rows->number of rows of the dataset\n",
    "#y->number of distinct words extracted from all over dataset\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7521</th>\n",
       "      <th>7522</th>\n",
       "      <th>7523</th>\n",
       "      <th>7524</th>\n",
       "      <th>7525</th>\n",
       "      <th>7526</th>\n",
       "      <th>7527</th>\n",
       "      <th>7528</th>\n",
       "      <th>7529</th>\n",
       "      <th>7530</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7531 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  7521  \\\n",
       "0     1     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     1     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     1     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   7522  7523  7524  7525  7526  7527  7528  7529  7530  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 7531 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will store the elements of features inside a variable x\n",
    "x=x.toarray()\n",
    "x=pd.DataFrame(x)\n",
    "#the dataset consists of fully numbers,since our ML model can not work with text data\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1    spam\n",
       "2     ham\n",
       "3     ham\n",
       "4     ham\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will store the value of label inside a variable y\n",
    "y=df[\"Label\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label\n",
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since this is categorical data so we need to convert it to numerical form\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Label Encoding means converting the whole categorical vale column to numerical form\n",
    "lb=LabelEncoder()\n",
    "y=lb.fit_transform(y)\n",
    "y=pd.DataFrame(y)\n",
    "y.columns=[\"Label\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_lor=LogisticRegression()\n",
    "model_lor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_lor=model_lor.predict(x_test)\n",
    "y_preds_lor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[954,   1],\n",
       "       [ 14, 145]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lor=confusion_matrix(y_test,y_preds_lor)\n",
    "cm_lor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm_lor,linewidth=1,linecolor=\"black\",cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9865350089766607"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_lor=accuracy_score(y_test,y_preds_lor)\n",
    "acc_lor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
